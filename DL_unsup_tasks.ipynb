{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_unsup-tasks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wildonion/uniXerr/blob/master/DL_unsup_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xQsz_rda-3X",
        "colab_type": "text"
      },
      "source": [
        "# **uniXerr PROJECT :** \n",
        "    RESOURCES, HELPERS & KITS\n",
        "![uniXerr logo](https://drive.google.com/uc?id=1TXJwfJsTJzU2M7LrIQgx2Tx4cfUzcQuX)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRUjWqr6pIf4",
        "colab_type": "text"
      },
      "source": [
        "[awesome capsule networks](https://github.com/sekwiatkowskiawesome-capsule-networks)\n",
        "\n",
        "[all GANs variation in keras](https://github.com/eriklindernoren/Keras-GAN)\n",
        "\n",
        "[Geoffrey Hinton course](https://github.com/khanhnamle1994/neural-nets)\n",
        "\n",
        "[feature scaling](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
        "\n",
        "[gans-cgans-ae-aae-ave-caae-cave](https://medium.com/lis-computer-vision-blogs/gans-cgans-ae-aae-ave-caae-cave-2e7d23255b52)\n",
        "\n",
        "[must read papers on gans](https://towardsdatascience.com/must-read-papers-on-gans-b665bbae3317)\n",
        "\n",
        "[MIT 6.S191 (2019): Deep Generative Modeling](https://www.youtube.com/watch?v=yFBFl1cLYx8&feature=youtu.be)\n",
        "\n",
        "[stylegans use machine learning to generate and customize realistic images](https://heartbeat.fritz.ai/stylegans-use-machine-learning-to-generate-and-customize-realistic-images-c943388dc672)\n",
        "\n",
        "[generative adversarial network by pathmind](https://pathmind.com/wiki/generative-adversarial-network-gan)\n",
        "\n",
        "[generating modern arts using generative adversarial network gan on spell](https://towardsdatascience.com/generating-modern-arts-using-generative-adversarial-network-gan-on-spell-39f67f83c7b4)\n",
        "\n",
        "[generating letters using generative adversarial networks](https://medium.com/ml-everything/generating-letters-using-generative-adversarial-networks-gans-161b0be3c229)\n",
        "\n",
        "[generative adversarial networks by toptal](https://www.toptal.com/machine-learning/generative-adversarial-networks)\n",
        "\n",
        "[adversarial autoencoders - researchcode](https://researchcode.com/code/1873018888/adversarial-autoencoders/)\n",
        "\n",
        "[intro to GAN by rubikscode](https://rubikscode.net/2018/12/10/introduction-to-generative-adversarial-networks-gans/)\n",
        "\n",
        "[intor to AAE by rubikscode](https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/)\n",
        "\n",
        "[GAN variations](https://towardsdatascience.com/gan-objective-functions-gans-and-their-variations-ad77340bce3c)\n",
        "\n",
        "[rubikscode - intro to autoencoders](https://rubikscode.net/2018/11/19/introduction-to-autoencoders/)\n",
        "\n",
        "[datacamp - autoencoders keras](https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial)\n",
        "\n",
        "[k-means clustering algorithm from scratch - machine learning](https://automaticaddison.com/k-means-clustering-algorithm-from-scratch-machine-learning/)\n",
        "\n",
        "[k-means clustering implementation](https://towardsdatascience.com/k-means-clustering-implementation-2018-ac5cd1e51d0a)\n",
        "\n",
        "[comprehensive guide k-means clustering](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/)\n",
        "\n",
        "[k-means clustering from scratch in python](https://medium.com/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d38eee42)\n",
        "\n",
        "[kmeans clustering from scratch - mubaris](https://mubaris.com/2017-10-01/kmeans-clustering-in-python)\n",
        "\n",
        "[k-means from scratch - dfrieds blog](https://dfrieds.com/machine-learning/k-means-from-scratch-python)\n",
        "\n",
        "[k-means from scratch](https://mmuratarat.github.io/2019-07-23/kmeans_from_scratch)\n",
        "\n",
        "[implementing k-means clustering from scratch](http://madhugnadig.com/articles/machine-learning/2017/03/04/implementing-k-means-clustering-from-scratch-in-python.html)\n",
        "\n",
        "[k-means clustering in python](http://benalexkeen.com/k-means-clustering-in-python/)\n",
        "\n",
        "[The variational auto-encoder](https://ermongroup.github.io/cs228-notes/extras/vae/)\n",
        "\n",
        "[Neural Networks and Clustering (Autoencoders)](https://www.youtube.com/watch?v=0oEMORg04zw&list=PLdk2fd27CQzRSl0UoxnMrlQY0uoQGmgLk)\n",
        "\n",
        "[K-Means Clustering - Methods using Scikit-learn in Python - Tutorial 23 in Jupyter Notebook](https://www.youtube.com/watch?v=ikt0sny_ImY)\n",
        "\n",
        "[autoencoder tutorial: machine learning with keras](https://www.youtube.com/watch?v=uCaPP4blYAg)\n",
        "\n",
        "[applying machine learning to classify an unsupervised text document](https://towardsdatascience.com/applying-machine-learning-to-classify-an-unsupervised-text-document-e7bb6265f52)\n",
        "\n",
        "[Unsupervised Machine Learning - Hierarchical Clustering with Mean Shift Scikit-learn and Python](https://www.youtube.com/watch?v=EQZaSuK-PHs)\n",
        "\n",
        "[Unsupervised Learning by siraj](https://www.youtube.com/watch?v=8dqdDEyzkFA)\n",
        "\n",
        "[understanding k-means clustering in machine learning](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n",
        "\n",
        "[comprehensive guide recommendation engine python](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/?utm_source=blog&utm_medium=comprehensive-guide-k-means-clustering)\n",
        "\n",
        "[build recommendation engine collaborative filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/)\n",
        "\n",
        "[k-means clustering unsupervised learning recommender systems](https://www.kdnuggets.com/2019/04/k-means-clustering-unsupervised-learning-recommender-systems.html)\n",
        "\n",
        "[build your own clustering based recommendation engine in 15 minutes](https://towardsdatascience.com/build-your-own-clustering-based-recommendation-engine-in-15-minutes-bdddd591d394)\n",
        "\n",
        "[machine learning for recommender systems](https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed)\n",
        "\n",
        "[machine learning recommender systems](https://www.kdnuggets.com/2019/09/machine-learning-recommender-systems.html)\n",
        "\n",
        "[comprehensive guide recommendation engine python](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/)\n",
        "\n",
        "[introduction to recommender systems](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)\n",
        "\n",
        "[Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/45530.pdf)\n",
        "\n",
        "[Building a Movie Recommendation Engine | Machine Learning Projects](https://www.youtube.com/watch?v=XoTwndOgXBM&t=5058s)\n",
        "\n",
        "[deep clustering](https://deepnotes.io/deep-clustering)\n",
        "\n",
        "[deep learning based clustering techniques](https://divamgupta.com/unsupervised-learning/2019/03/08/an-overview-of-deep-learning-based-clustering-techniques.html)\n",
        "\n",
        "[kohonen self organizing maps](https://towardsdatascience.com/kohonen-self-organizing-maps-a29040d688da)\n",
        "\n",
        "[recommendation system algorithms](https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3)\n",
        "\n",
        "[deep clustering for financial market segmentation](https://towardsdatascience.com/deep-clustering-for-financial-market-segmentation-2a41573618cf)\n",
        "\n",
        "[unsupervised deep learning computer vision](https://www.analyticsvidhya.com/blog/2018/06/unsupervised-deep-learning-computer-vision/)\n",
        "\n",
        "[Supervised and Unsupervised Learning In Machine Learning | Machine Learning Tutorial | Simplilearn](https://www.youtube.com/watch?v=kE5QZ8G_78c)\n",
        "\n",
        "[Unsupervised Machine Learning: Crash Course Statistics #37](https://www.youtube.com/watch?v=IUn8k5zSI6g)\n",
        "\n",
        "[unsupervised learning and data clustering](https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a)\n",
        "\n",
        "[autoencoders neural networks for unsupervised learning](https://medium.com/intuitive-deep-learning/autoencoders-neural-networks-for-unsupervised-learning-83af5f092f0b)\n",
        "\n",
        "[unsupervised clustering with keras](https://www.dlology.com/blog/how-to-do-unsupervised-clustering-with-keras/)\n",
        "\n",
        "[introduction to unsupervised learning by algorithmia](https://algorithmia.com/blog/introduction-to-unsupervised-learning)\n",
        "\n",
        "[building autoencoders in keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
        "\n",
        "[sparse stacked and variational autoencoder](https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64)\n",
        "\n",
        "[essentials of deep learning trudging into unsupervised deep learning](https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djqnizAEWY5O",
        "colab_type": "text"
      },
      "source": [
        "# **Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4fsxaaVdgGy",
        "colab_type": "code",
        "outputId": "d7b4a086-0edf-477a-fae2-057e6d243d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "\n",
        "# Importing Modules - run twice!\n",
        "\n",
        "from __future__ import print_function, division\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')\n",
        "    from PIL import Image\n",
        "    import numpy as np, pandas as pd, matplotlib.pyplot as plt, os, sys, imageio, tensorflow as tf, plotly.graph_objects as go, hdbscan, cv2, asyncio, math\n",
        "    from tensorflow.keras.layers import MaxPooling2D, Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2DTranspose, Conv2D\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "    from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "except:\n",
        "    print(\"\\n\\n[!] Installing Dependencies...\")\n",
        "    !pip install hdbscan\n",
        "    !pip install tqdm\n",
        "finally:\n",
        "    print(\"\\n\\n[+] All Modules Loaded Successfully!\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "[+] All Modules Loaded Successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIByIA3_VywB",
        "colab_type": "text"
      },
      "source": [
        "**Building the numpy pixels of all art images in google drive**\n",
        "\n",
        "> `run below cell whenever you have new data in gdrive art folder`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoSHHyQtVgOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildPaint():\n",
        "    training_data = []\n",
        "    filenames = os.listdir('/gdrive/My Drive/art/')\n",
        "    for fname in filenames:\n",
        "        image_path = os.path.join('/gdrive/My Drive/art/'+fname)\n",
        "        image = Image.open(image_path).resize((128,128), Image.ANTIALIAS)\n",
        "        # plt.imshow(image)\n",
        "        training_data.append(np.asarray(image))\n",
        "        # print(image)\n",
        "    np.save('/gdrive/My Drive/art/paint.npy', training_data)\n",
        "\n",
        "\n",
        "buidPaint()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cq4r_9PWMKN",
        "colab_type": "text"
      },
      "source": [
        "# **An Image Processing Kit for uniXerr Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFbUpUy-cayI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class IPKit:\n",
        "    '''\n",
        "    Image Processing Kit for Deep Learning Models\n",
        "\n",
        "    HELPERS:\n",
        "        https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays\n",
        "        https://colab.research.google.com/drive/1W79kelri9bXpLsnMtas79fmzLIgWC5bN#scrollTo=aZCfZlN42gfr&line=2&uniqifier=1\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.drivePath = '/gdrive/My Drive/'\n",
        "\n",
        "    def saveGANIMG(self, generated, epoch):\n",
        "        if not os.path.isdir('generated'): os.mkdir('generated') \n",
        "        fig, axs = plt.subplots(5, 5) # (5 , 5) images for 25 noises\n",
        "        batch_count = 0\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                axs[row, col].imshow(generated[batch_count, :, :, 0] * 127.5 + 127.5, cmap='gray') # plot image on each of 25 axis of figure object\n",
        "                axs[row, col].axis('off') # hide the related axis\n",
        "                batch_count += 1 # get ready for next data row\n",
        "        fig.savefig(f\"generated/{epoch}.png\")\n",
        "        plt.close()\n",
        "    \n",
        "    def MakeGif(self):\n",
        "        filenames = [ fname for fname in np.sort(os.listdir('generated')) if \".png\" in fname]\n",
        "        with imageio.get_writer('generated/dcgan.gif', mode=\"I\") as writer: # open a writer object for writing images on it to export a gif\n",
        "            for filename in filenames: # for every file in filenames list read them\n",
        "                image = imageio.imread('generated/'+filename)\n",
        "                writer.append_data(image) # append opened image into writer object for making gif\n",
        "    \n",
        "    def loadPaint(self):\n",
        "        return np.load(os.path.join(self.drivePath+'art/paint.npy'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4o089DQ1_Kq",
        "colab_type": "text"
      },
      "source": [
        "# **uniXerr authSys NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGT-E3vc2GhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "------------------------------------------------------------\n",
        " DOUBLE AUTHENTICATING PREVENTER USING RNNs AND BLOCKCHAIN\n",
        "------------------------------------------------------------\n",
        "                |p2p network for blockchain|\n",
        "\n",
        "\n",
        "\n",
        "        |=> check a user data by using ai to see that he/she is already logged in or not : verifying authenticity! <=|\n",
        "        |=> after validating each user they will mine and add to the blockchain ledger by this network for later usage of epcbm network <=|\n",
        "        |=> after uniXerr users are added to the blockchain ledger the uniXerr protocol will give 10 coins to everyone by default <=| \n",
        "        |=> new predictions (generated by epcbm NETWORK) for each user will add to the uniXerr blockchain ledger for later predictions <=|\n",
        "        |=> to mine a block (add users predictions to a block) we use an ai algorithm like clustering to verify the predictions and user authenticity validation <=|\n",
        "        |=> after the blocks are mined and added to the chain with no problem, predicted coins by epcbm NETWORK will give to each user <=|\n",
        "        |=> finally we'll sotore the whole blockchain in a mongo database <=|\n",
        "\n",
        "\n",
        "        https://towardsdatascience.com/traditional-vs-deep-learning-algorithms-used-in-blockchain-in-retail-industry-iii-ed873186b6e9\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZELpcRfvGDYE",
        "colab_type": "text"
      },
      "source": [
        "# **uniXerr getClus NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx6Ivr0CDLF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "--------------------------------------------------------------\n",
        "  CLUSTERING USING HDBSCAN & AUTOENCODERS BASED ON DEC METHOD\n",
        "--------------------------------------------------------------\n",
        "                    |min_cluster_size : 20|\n",
        "                          \n",
        "\n",
        "                        \n",
        "        cluster1 : A1               cluster16 : D1\n",
        "        cluster2 : A2               cluster17 : D2\n",
        "        cluster3 : A3   .   .   .   cluster18 : D3\n",
        "        cluster4 : A4               cluster19 : D4\n",
        "        cluster5 : A5               cluster20 : D5\n",
        "\n",
        "\n",
        "        clusters > D5 => will get a warning and high recommendation using \n",
        "                                uniXerr recSys NETWORK to improve their \n",
        "                                    study skills and increase the efforts.\n",
        "\n",
        "        \n",
        "                |=> after clustering we have labels for our students to solve the epcbm classification problem <=|\n",
        "\n",
        "        https://arxiv.org/pdf/1806.02146v1.pdf\n",
        "        https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e\n",
        "        https://www.kaggle.com/aljarah/xAPI-Edu-Data\n",
        "        https://www.pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/\n",
        "        https://www.kaggle.com/spscientist/students-performance-in-exams\n",
        "        https://archive.ics.uci.edu/ml/datasets/student+performance\n",
        "        https://www.ijrte.org/wp-content/uploads/papers/v8i1s4/A11880681S419.pdf\n",
        "        https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "        https://hdbscan.readthedocs.io/en/latest/index.html\n",
        "        https://github.com/XifengGuo/DEC-keras\n",
        "        https://www.dlology.com/blog/how-to-do-unsupervised-clustering-with-keras/\n",
        "        https://ai-mrkogao.github.io/reinforcement%20learning/clusteringkeras/\n",
        "        http://www.datastuff.tech/machine-learning/autoencoder-deep-learning-tensorflow-eager-api-keras/\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zy9U3-nbzWq",
        "colab_type": "text"
      },
      "source": [
        "# **uniXerr epcbm NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXTBat_TFX7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "----------------------------------------------------------------------------------------------\n",
        "  ESTIMATE & PREDICT THE POSITIONS , COINS , BEHAVIOUR ACTIVITY , SEMESTER STATUS & MARKS \n",
        "    FOR uniXerr USERS BASED ON THEIR PAST EVENTS AND CLUSTER USING RNNs (LSTM/GRU) & GAN (AAE)\n",
        "----------------------------------------------------------------------------------------------\n",
        "                                  |natural language processing|\n",
        "\n",
        "\n",
        "        https://www.youtube.com/watch?v=_iag_If4yYA\n",
        "        https://www.youtube.com/watch?v=ULWLleBDCEY\n",
        "        https://www.youtube.com/watch?v=xvqsFTUsOmc\n",
        "        https://medium.com/cindicator/music-generation-with-neural-networks-gan-of-the-week-b66d01e28200\n",
        "        https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571\n",
        "        https://xiandong79.github.io/seq2seq-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86\n",
        "        https://galaxyproject.github.io/training-material/topics/statistics/tutorials/age-prediction-with-ml/tutorial.html\n",
        "        https://medium.com/@chataks93/predicting-human-behaviour-activity-using-deep-learning-lstm-fff9030b82e7\n",
        "        https://arxiv.org/pdf/1708.08744.pdf\n",
        "        https://arxiv.org/pdf/1804.07405.pdf\n",
        "        https://www.ritchieng.com/machine-learning-project-student-intervention/\n",
        "        https://towardsdatascience.com/predicting-students-grades-on-kaggle-fd6ac9b1bfb9\n",
        "\n",
        "\n",
        "        EX :\n",
        "                    [hey wildonion]\n",
        "                weekly positon                           : [A.....B..*..C.....D]\n",
        "                total mined coins                        : 45\n",
        "                estimated position                       : A scope\n",
        "                estimated coin to mine                   : 10\n",
        "                estimated marks for next quizzes         : [NLA : 80 , CSA : 45 , MATH-1 : 78]\n",
        "                estimated mark for semester courses exam : [NLA : 43 , CSA : 23 , MATH-1 : 34]\n",
        "                monthly estimated position               : C scope\n",
        "                total semester estimated position        : B scope\n",
        "                interests                                : parkour , music-rock , paiting \n",
        "                characteristic                           : generosity , integrity and loving\n",
        "                age                                      : 22\n",
        "                semester status                          : pass\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUPa00I3qDR8",
        "colab_type": "text"
      },
      "source": [
        "# **uniXerr recSys NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_LBxsTsqJjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "--------------------------------------------------------------------------\n",
        "  HYBRID CONTENT COLLABORATIVE recSys USING ADVERSARIAL AUTOENCODERS (AAE)\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "        |=> use all personal infos inside users phone for mining process like location, mobile model and etc. <=|\n",
        "        |=> We don’t want the same input and thus we take a sample from latent vector and put into the decoder to give similar output. <=|\n",
        "\n",
        "\n",
        "        https://www.youtube.com/watch?v=u1bpD8Lgp8E\n",
        "        https://colab.research.google.com/drive/1OOttboitAThA2FKAsUKwJQlMfrdRxVlH#scrollTo=Z-JIkbOWCNt3\n",
        "        https://developers.google.com/machine-learning/glossary/recsystemshttps\n",
        "        https://www.youtube.com/watch?v=y_TzOOCJqxI\n",
        "        https://pdfs.semanticscholar.org/dfad/b5e2e274d3b2de2c203178b5b30cb7a7d0db.pdf\n",
        "        http://ceur-ws.org/Vol-2367/paper_2.pdf\n",
        "        https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "        https://towardsdatascience.com/creating-a-hybrid-content-collaborative-movie-recommender-using-deep-learning-cc8b431618af\n",
        "        https://github.com/SudharshanShanmugasundaram/Movie-Recommendation-System-using-AutoEncoders\n",
        "        https://medium.com/snipfeed/how-variational-autoencoders-make-classical-recommender-systems-obsolete-4df8bae51546\n",
        "        https://towardsdatascience.com/deep-autoencoders-for-collaborative-filtering-6cf8d25bbf1d\n",
        "\n",
        "\n",
        "        EX : \n",
        "                [hey wildonion]\n",
        "            recommended users near to your position  : @alexa , @rosii, @force\n",
        "            suggestion based to you                  : books, music, movies, teachers, users, courses\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcO2lDozww-O",
        "colab_type": "text"
      },
      "source": [
        "# **uniXerr recog NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy0tnXROw5wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "---------------------------------------------------------------------\n",
        "  IMAGE PROCESSING AND OBJECT DETECTION USING CNN , YOLO & CapsuleNet\n",
        "---------------------------------------------------------------------\n",
        "           |text <-> speech recognition & image processing|\n",
        "\n",
        "\n",
        "        https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e\n",
        "        https://www.intechopen.com/online-first/object-recognition-using-convolutional-neural-networks\n",
        "        https://machinelearningmastery.com/introduction-to-deep-learning-for-face-recognition/\n",
        "        https://towardsdatascience.com/how-to-build-a-face-detection-and-recognition-system-f5c2cdfbeb8c\n",
        "        https://towardsdatascience.com/face-recognition-using-artificial-intelligence-fffa3b20ad5f\n",
        "\n",
        "        EX : login with face detection.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2DZ-_zXl9wV",
        "colab_type": "text"
      },
      "source": [
        "# **[uniXerr APIs](https://github.com/wildonion/uniXerr)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3rrxGUmmC0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - import above models here after they are trained to create graphql API using tensorflow serving (tfx) and flask\n",
        "# TODO - store the data in uniXerr blockchain after they are mined using authSys NETWORK then load the blockchain in mongo database.\n",
        "# TODO - configure arvancloud vps firewall and load balancers like nginx/traefik/CDN (reverse proxy) and anaconda uniXerr env using systemd, pm2 and docker(k8s)\n",
        "# TODO - a compiled angel-dart.dev/aqueduct.io cli app using public/private keys for uniXerr servers conf and shutting down from every where using pm2\n",
        "# TODO - compile this code cell using cython to have a fully controlled uniXerr app ; like a backdoor ; using a single compiled file\n",
        "# NOTE - ai.py ideas to complete uniXerr blog posts, deepkit.ai, tensorboard for visualization to build and complete this project\n",
        "\n",
        "\n",
        "''' resources :\n",
        "        https://medium.com/@marvinkome/creating-a-graphql-server-with-flask-ae767c7e2525\n",
        "        https://graphene-mongo.readthedocs.io/en/latest/tutorial.html\n",
        "        https://bcb.github.io/python/graphql-flask\n",
        "        https://fluttercrashcourse.com/blog/\n",
        "        https://www.youtube.com/watch?v=OVG3L5JcEOc\n",
        "        https://www.youtube.com/watch?v=5x6S4kDODa8\n",
        "        https://www.youtube.com/watch?v=YAWk2mjtqQE\n",
        "        https://www.youtube.com/watch?v=iPbM10mvpko\n",
        "        https://www.youtube.com/watch?v=ggmjumYogEo\n",
        "        https://www.youtube.com/watch?v=UzuecP3utk8\n",
        "        https://www.youtube.com/watch?v=pTJJsmejUOQ\n",
        "        https://www.youtube.com/watch?v=XHsrxgoESz8\n",
        "        https://github.com/iampawan/Flutter-UI-Kit\n",
        "        https://github.com/flutter/samples\n",
        "        https://www.youtube.com/watch?v=CSa6Ocyog4U\n",
        "        https://www.youtube.com/watch?v=GLSG_Wh_YWc\n",
        "        https://www.youtube.com/watch?v=x0uinJvhNxI\n",
        "        https://paulwelch.dev/2017/12/07/running-tensorflow-p2.html\n",
        "        https://guillaumegenthial.github.io/serving.html\n",
        "        https://towardsdatascience.com/deploying-keras-deep-learning-models-with-flask-5da4181436a2\n",
        "        https://towardsdatascience.com/deploying-keras-models-using-tensorflow-serving-and-flask-508ba00f1037\n",
        "        https://medium.com/tensorflow/serving-ml-quickly-with-tensorflow-serving-and-docker-7df7094aa008\n",
        "        https://medium.com/repro-repo/deploy-a-trained-rnn-lstm-model-with-tensorflow-serving-and-flask-part-1-introduction-and-336dbb8f02f\n",
        "        https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uswgi-and-nginx-on-ubuntu-18-04\n",
        "        https://dart.academy/building-a-real-time-chat-app-with-angel-and-flutter/\n",
        "        https://thosakwe.com/developing-restful-apis-with-angel/\n",
        "        https://thosakwe.com/deploying-dart-apps-to-linux/\n",
        "        https://thosakwe.com/aot-compilation-and-other-dart-hackery/\n",
        "        https://becominghuman.ai/creating-restful-api-to-tensorflow-models-c5c57b692c10\n",
        "        https://medium.com/flutter-community/dart-aqueduct-server-for-your-flutter-app-part-1-getting-started-547e098196d1\n",
        "        https://blog.tanka.la/2018/07/15/deploy-your-first-deep-learning-neural-network-model-using-flask-keras-tensorflow-in-python/\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzuZYGjseftQ",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Capsule & Deep Convolutional GAN**\n",
        "\n",
        "**Deep Convolutioanl GAN  several improvements:**\n",
        "\n",
        "*   Utilizing the convolution layer instead pooling function in the Discriminator\n",
        "model for reducing dimensionality. This way, the network itself will learn how to reduce dimensionality. On the other hand, in the Generator Model, we use deconvolution to upsample dimensions of feature maps.\n",
        "*   Adding in the batch normalization. This is used to increase the stability of a neural network. In an essence, batch normalization normalizes the output of a previous layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
        "*   Remove fully connected layers from Convolutional Neural Network.\n",
        "*   Use Relu and Leaky Relu activation functions.\n",
        "\n",
        "![DCGAN job](https://drive.google.com/uc?id=1Ind08ydejfh6IYYl6Gw_jYfLEGN4Eiph)\n",
        "\n",
        "![DCGAN process](https://drive.google.com/uc?id=1wZufkk6jq22l15a8VUFEQ1MT5mfERMS_)\n",
        "\n",
        "**Discriminator Process**\n",
        "\n",
        "> Strided convolution instead of max-pooling down samples the image.\n",
        "\n",
        "![D_process](https://drive.google.com/uc?id=1mQSjU2KVzOEQwx5qdp7VTglx5AvUhd_3)\n",
        "\n",
        "**Generator Process**\n",
        "\n",
        "> Upsampling is used instead of fractionally-strided transposed convolution.\n",
        "\n",
        "![G_process](https://drive.google.com/uc?id=1AWKUP8dGW8xdXVX8JENavoBR_JSR01WA)\n",
        "\n",
        "**Adversarial Network**\n",
        "\n",
        "> The Adversarial model is simply generator with its output connected to the input of the discriminator. Also shown is the training process wherein the Generator labels its fake image output with 1.0 trying to fool the Discriminator.\n",
        "\n",
        "![G_process](https://drive.google.com/uc?id=1jMhMV5kiaCqNa9x1E-YDdTTB4CWB8XOD)\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "> Discriminator in GAN uses a cross entropy loss, since discriminators job is to classify; cross entropy loss is the best one out there.\n",
        "\n",
        "![gan loss](https://drive.google.com/uc?id=1TZlEihIaUqK4v8_MFYb8Ilf9o0Rjw2PR)\n",
        "\n",
        "> This formula represents the cross entropy loss between `p`: the true distribution and `q`: the estimated distribution.\n",
        "`(p)` and `(q)` are the of m dimensions where m is the number of classes.\n",
        "\n",
        "![cross entropy](https://drive.google.com/uc?id=1BJSC-RUODhllXGDR6TnzuRkYBwKjg0xF)\n",
        "\n",
        "> In GAN, discriminator is a binary classifier. It needs to classify either the data is real or fake. Which means `m = 2`. The true distribution is one hot vector consisting of only 2 terms.\n",
        "For `n` number of samples, we can sum over the losses.\n",
        "This above shown equation is of binary cross entropy loss, where `y` can take two values 0 and 1.\n",
        "GAN’s have a latent vector `z`, image `G(z)` is magically generated out of it. We apply the discriminator function `D` with real image `x` and the generated image `G(z)`.\n",
        "The intention of the loss function is to push the predictions of the real image towards 1 and the fake images to 0. We do so by log probability term.\n",
        "\n",
        "![minmax formula](https://drive.google.com/uc?id=1Ky3cfOdWT1tRNk3SLT7Luscko1e3J0NT)\n",
        "\n",
        "**Note:** `~` sign means: is distributed as and Ex here means expectations: since we don’t know how samples are fed into the discriminator, we are representing them as expectations rather than the sum.\n",
        "If we observe the joint loss function we are maximizing the discriminator term, which means log of `D(x)` should inch closer to zero, and log `D(G(z))` should be closer to 1. Here generator is trying to make `D(G(z))` inch closer to 1 while discriminator is trying to do the opposite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWY7smdQiNwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "'''\n",
        "https://github.com/gusgad/capsule-GAN/blob/master/inception_score.ipynb\n",
        "https://github.com/gusgad/capsule-GAN/blob/master/capsule_gan.ipynb\n",
        "https://github.com/XifengGuo/CapsNet-Keras\n",
        "https://software.intel.com/en-us/articles/understanding-capsule-network-architecture\n",
        "'''\n",
        "\n",
        "\n",
        "class GAN:\n",
        "    '''\n",
        "    we convert 60000 training images to float32 type\n",
        "    then normalize and scale the pixel data by half of the 255:\n",
        "        the activation function of the output layer of the generator is tanh, \n",
        "        which returns a value between -1 and 1. To scale that to 0 and 255 \n",
        "        (which are the values you expect for an image), we have to multiply it \n",
        "        by 127.5 (so that -1 becomes -127.5, and 1 becomes 127.5), and then \n",
        "        add 127.5 (so that -127.5 becomes 0, and 127.5 becomes 255). We then \n",
        "        have to do the inverse of this when feeding an image into the \n",
        "        discriminator (which will expect a value between -1 and 1).\n",
        "    \n",
        "    Leaky ReLUs are one attempt to fix the “dying ReLU” problem. \n",
        "    Instead of the function being zero when x < 0, a leaky ReLU \n",
        "    will instead have a small negative slope (of 0.01, or so). \n",
        "    That is, the function computes f(x)=1(x<0)(αx)+1(x>=0)(x) where α is a small constant.\n",
        "\n",
        "            NOTE : the None in models' summary is the batch dimension.\n",
        "            NOTE : \"same\" results in padding the input such that the output has the same length as the original input.\n",
        "\n",
        "            Resources :\n",
        "                https://www.freecodecamp.org/news/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc/\n",
        "                https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
        "                https://keras.io/getting-started/functional-api-guide/\n",
        "                https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\n",
        "    '''\n",
        "    def __init__(self, dataset_name='paint', channels=3, generator_input_features=100, discNetwork='dcgan'):\n",
        "            self.channels = channels # the default is 3, because paint dataset has colorful images \n",
        "            self.ipkit = IPKit()\n",
        "            self.dcganFlag, self.capsuleFlag = False, False \n",
        "            \n",
        "            if dataset_name == 'mnist' or dataset_name == 'fashion_mnist':\n",
        "                self.h, self.w = 28, 28\n",
        "                self.mDep = int(self.h/4)\n",
        "\n",
        "            if discNetwork == 'dcgan':\n",
        "                self.dcganFlag = True\n",
        "            if discNetwork == 'cgan':\n",
        "                self.capsuleFlag = True\n",
        "            \n",
        "            if dataset_name == 'mnist':\n",
        "                (self.x_train, _), (_, _) = mnist.load_data()\n",
        "                self.x_train = np.expand_dims(self.x_train, axis=3) # shape : (60000, 28, 28, 1) ; you can also use self.x_train.reshape(self.x_train.shape[0], self.h, self.w, 1)\n",
        "            elif dataset_name == 'fashion_mnist':\n",
        "                (self.x_train, _), (_, _) = fashion_mnist.load_data()\n",
        "                self.x_train = np.expand_dims(self.x_train, axis=3) # shape : (60000, 28, 28, 1) ; you can also use self.x_train.reshape(self.x_train.shape[0], self.h, self.w, 1)\n",
        "            elif dataset_name == 'paint':\n",
        "                self.h, self.w = 128, 128\n",
        "                self.mDep = int(self.h/32)\n",
        "                self.x_train = self.ipkit.loadPaint()\n",
        "                self.x_train = np.reshape(self.x_train, (self.x_train.shape[0], self.h, self.w, self.channels))\n",
        "\n",
        "            self.x_train = (self.x_train.astype(np.float32) - 127.5)/127.5 # normalize the images to [-1, 1] - because the output of our generator is squashed by a tanh activation function which give a number in range [-1, 1] \n",
        "            self.generator_input_features = generator_input_features\n",
        "            self.discriminator_input = self.x_train[0].shape # (128, 128, 3) for paint dataset\n",
        "            self.__create_networks()\n",
        "\n",
        "\n",
        "    def __MakeGeneratorModel(self):\n",
        "        '''\n",
        "        creating generator layers activated by tanh.\n",
        "        basically this model generates noisy images for first rounds and real images at the end of total epochs.\n",
        "        \n",
        "        The generator model is typically implemented using a deep convolutional neural network \n",
        "        and results-specialized layers that learn to fill in features in an image \n",
        "        rather than extract features from an input image, cause we want to produce a real image\n",
        "        from noisy one by learning the features map (deconvolutional process).\n",
        "\n",
        "        GAN architecture are required to upsample input data in order since it synthesizes more realistic images.\n",
        "\n",
        "        fractional stride (deconvolutional layers) can be used in the generator for upsampling.\n",
        "        \n",
        "        The upsampling layer is a simple layer with no weights that will double the dimensions of \n",
        "        input and can be used in a generative model when followed by a traditional convolutional layer.\n",
        "\n",
        "        NOTE : in order to understand the architecture of the generator model see its summary and the shape of training images!\n",
        "        NOTE : the output shape of Conv2DTranspose with padding same is : output = input * stride\n",
        "        NOTE : the default stride is 1 because we choosed to use UpSampling2D layer.\n",
        "        NOTE : you can comment UpSampling2D layers, set strides=2 for each Conv2DTranspos layers to get the same result with \n",
        "               UpSampling2D layer cause transposed convolutions are more flexible than classical upsampling methods.\n",
        "        NOTE : default strides option of Conv2DTranspose layer doesn't affect the output shape because the argument is set to 1 by default and we have UpSampling2D layer.\n",
        "               you can uncomment the Upsampling2D layers and comment Conv2DTranspose layers or set the strides of each Conv2DTranspose layers to 1 to get the benefits of upsampling method.\n",
        "        '''\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ GENERATOR SUMMARY - FEATURES/NEURONS/INPUTS STRUCTURE +)+)+)+)+)+) \\n\\n\")\n",
        "        generator_input_features = Input(shape=(self.generator_input_features,)) # create a Input layer with size for example 100 (first layer neurons)\n",
        "        self.generator = Sequential() # create sequential model object \n",
        "        self.generator.add(Dense(HPARAM.depth * self.mDep * self.mDep, input_dim=self.generator_input_features)) # size of next layer (hidden) is (None, HPARAM.depth * self.mDep * self.mDep) with the input : (None, 100) - weights matrix size : (100, HPARAM.depth * self.mDep * self.mDep)\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        self.generator.add(Reshape((self.mDep, self.mDep, HPARAM.depth))) # reshape to (None, self.mDep, self.mDep, 256) - None is the batch size dim\n",
        "        self.generator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting input features to zero for perviouse layer to prevent over-fitting\n",
        "        # self.generator.add(UpSampling2D()) # opposite of pooling layer - doubles the dimensions of the last layer output ; output size : (None, 2*self.mDep, 2*self.mDep, 256)\n",
        "        \n",
        "        for n_layer in range(int(math.log2(self.h/self.mDep))):\n",
        "            self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth), kernel_size=5, strides=2, padding='same')) # output size : double the pervious layer output in every iteration by strides with filter 256\n",
        "            self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.generator.add(LeakyReLU())\n",
        "            # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/2), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 128) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 128 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/4), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 64) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 64 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/8), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 32) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 32 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        # self.generator.add(UpSampling2D()) # double the last output size, not the filter!\n",
        "        self.generator.add(Conv2DTranspose(filters=int(HPARAM.depth/16), kernel_size=5, padding='same')) # output size : (None, self.h, self.w, 16) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with 16 filters\n",
        "        self.generator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "        self.generator.add(LeakyReLU())\n",
        "        self.generator.add(Conv2DTranspose(filters=self.channels, kernel_size=5, padding=\"same\")) # image channels as the number of filters of the last layer - output size : (None, self.h, self.w, self.channels) with padding \"same\" after convolutional ops >>>> input_width & input_height = self.h * strides with self.channels filters\n",
        "        self.generator.add(Activation(\"tanh\")) # -1 < output < 1\n",
        "        self.generator.summary()\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ GENERATOR MODEL SUMMARY AFTER TURNING IT INTO A TENSOR +)+)+)+)+)+) \\n\\n\")\n",
        "        generator_output_tensor = self.generator(generator_input_features) # turn our generator sequential model object into a tensor with input layer for example 100 neurons - output size : (None, self.h, self.w, self.channels)\n",
        "        self.generator_model = Model(generator_input_features, generator_output_tensor) # create the generator model with for example 100 inputs and (None, self.h, self.w, self.channels) output\n",
        "        self.generator_model.compile(loss=HPARAM.loss, optimizer=HPARAM.optimizer('Adam'), metrics=HPARAM.metrics) # binary crossentropy between an output (predicted y) tensor and a target (real y) tensor.\n",
        "        self.generator_model.summary()\n",
        "        print(f\"\\n\\n\\t\\t [======Generator Tensor======] \\n\\n\\t\\t {generator_output_tensor}\\n\\n\")\n",
        "\n",
        "    def __MakeDiscriminatorModel(self):\n",
        "        '''\n",
        "        The discriminator model takes an example from the domain as input (real or generated)\n",
        "        and predicts a binary class label of real or fake (generated).\n",
        "        \n",
        "        we use downsampling in the discriminator model to reduce dimensionality.\n",
        "        \n",
        "        In GANs, the recommendation is to not use pooling layers, \n",
        "        and instead use the stride in convolutional layers to \n",
        "        perform downsampling in the discriminator model.\n",
        "        \n",
        "        for the output layer we'll use sigmoid activation function to\n",
        "        squashes the output to a range between 0 and 1 for discriminating images.\n",
        "\n",
        "        NOTE : in order to understand the architecture of the discriminator model see its summary and the shape of training images!\n",
        "        NOTE : the output shape of Conv2D with padding same is : output = input // stride\n",
        "        NOTE : Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', strides=2)\n",
        "               if padding == 'same':\n",
        "                   output_length = input_length\n",
        "               elif padding == 'valid':\n",
        "                   output_length = input_length - filter_size\n",
        "               return (output_length + stride - 1) // stride\n",
        "               so (input=400 + 2-1)//2 = (200,200) will be (H,W) respectively and including filter (200,200,filter)\n",
        "        NOTE : you can remove the strides argument from each Conv2D layer and use MaxPooling2D with pool_size=2 layer to half the size of the width and height of the input features.\n",
        "               just remember to use a MaxPooling2D layer as the first layer of the discriminator functional model to half the size of the input features : maxpooling((None, self.h, self.w, self.channels)) -> Conv2D(32, 5, \"same\") -> (None, self.h/2, self.w/2, 32)\n",
        "        '''\n",
        "        discriminator_input = Input(shape=self.discriminator_input) # create the input layer with size for example (self.h, self.w, self.channels)\n",
        "        self.discriminator = Sequential()\n",
        "        if self.dcganFlag:\n",
        "            print(\"\\n\\n (+(+(+(+(+(+ DEEP CONVOLUTIONAL DISCRIMINATOR SUMMARY - FEATURES/NEURONS/INPUTS STRUCTURE +)+)+)+)+)+) \\n\\n\")\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/16), kernel_size=5, strides=2, input_shape=self.discriminator_input, padding=\"same\")) # output size : (None, self.h/2, self.w/2, 16) of first hidden layer - input size : (None, self.h, self.w, self.channels) >>> input_width & input_height = self.h / strides with 16 filters \n",
        "            self.discriminator.add(LeakyReLU(0.2)) # fix the “dying ReLU” problem by alpha = 0.2\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting inputs features to zero for perviouse layer and to each element or cell within the feature maps\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/8), kernel_size=5, strides=2, padding=\"same\")) # output size : half the pervious layer output by strides with 32 filters\n",
        "            if self.h == 28:\n",
        "                self.discriminator.add(ZeroPadding2D(padding=((0,1),(0,1)))) # add rows and columns of zeros at the top, bottom, left and right side of an image tensor - output size : (None, (self.h/4)+1, (self.w/4)+1, 32) \n",
        "            \n",
        "            if self.h == 128:\n",
        "                for n_layer in range(2):\n",
        "                    self.discriminator.add(Conv2D(filters=int(HPARAM.depth/8), kernel_size=5, strides=2, padding=\"same\")) # output size : half the pervious layer output in every iteration by strides with 32 filters\n",
        "                    self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "                    self.discriminator.add(LeakyReLU(0.2))\n",
        "                    self.discriminator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting input features to zero for perviouse layer and to each element or cell within the feature maps to prevent over-fitting \n",
        "\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/4), kernel_size=5, strides=2, padding=\"same\")) # output size : (None, 4, 4, 64) \n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout)) \n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/4), kernel_size=5, strides=2, padding=\"same\")) # output size : (None, 2, 2, 64) \n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout))\n",
        "            self.discriminator.add(Conv2D(filters=int(HPARAM.depth/2), kernel_size=5, strides=1, padding=\"same\")) # output size : (None, 2, 2, 128)\n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout))\n",
        "            self.discriminator.add(Conv2D(filters=HPARAM.depth, kernel_size=5, strides=1, padding=\"same\")) # output size : (None, 2, 2, 256) >>>\n",
        "            self.discriminator.add(BatchNormalization(momentum=HPARAM.momentum))\n",
        "            self.discriminator.add(LeakyReLU(0.2))\n",
        "            self.discriminator.add(Dropout(HPARAM.dropout)) # apply a dropout with a 40% chance of setting inputs features to zero for perviouse layer and to each element or cell within the feature maps\n",
        "            self.discriminator.add(Flatten()) # turn the last layer by flatten it into a fully dense connected for prediction in the next - output size : (None, 1024)\n",
        "            self.discriminator.add(Dense(1, activation='sigmoid')) # one neuron (single scalar) at the output ; means the image is real or fake, 1 for real (if the sigmoid neuron's output is larger than or equal to 0.5) and 0 for fake (if the output is smaller than 0.5) - output size : (None, 1) | weights matrix size : (1024, 1)\n",
        "            self.discriminator.summary()\n",
        "        if self.capsuleFlag:\n",
        "            print(\"\\n\\n (+(+(+(+(+(+ CAPSULE DISCRIMINATOR SUMMARY - FEATURES/NEURONS/INPUTS STRUCTURE +)+)+)+)+)+) \\n\\n\")\n",
        "            # ...\n",
        "\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ DISCRIMINATOR MODEL SUMMARY AFTER TURNING IT INTO A TENSOR +)+)+)+)+)+) \\n\\n\")\n",
        "        discriminator_tensor = self.discriminator(discriminator_input) # turn our discriminator sequential model object into a tensor with an input layer for example (None, self.h, self.w, self.channels) inputs or neurons - output size : (None , 1)\n",
        "        self.discriminator_model = Model(discriminator_input, discriminator_tensor) # create the discriminator model with for example (None, self.h, self.w, self.channels) inputs and (None, 1) output - one input and one sequential object\n",
        "        self.discriminator_model.compile(loss=HPARAM.loss, optimizer=HPARAM.optimizer('Adam'), metrics=HPARAM.metrics) # binary crossentropy between an output (predicted y) tensor and a target (real y) tensor since the output of the discriminator is sigmoid \n",
        "        self.discriminator_model.summary()\n",
        "        print(f\"\\n\\n\\t\\t [======Discriminator Tensor======] \\n\\n\\t\\t {discriminator_tensor}\\n\\n\")\n",
        "\n",
        "    def __create_networks(self):\n",
        "        '''\n",
        "        We now create the GAN where we combine the Generator and discriminator. \n",
        "        When we train the generator we will freeze the discriminator model.\n",
        "        \n",
        "        We will input the noised image of shape for example 100 units to the generator. \n",
        "        The output generated from the generator will be fed to the discriminator.\n",
        "        '''\n",
        "        self.__MakeGeneratorModel()\n",
        "        self.__MakeDiscriminatorModel()\n",
        "        print(\"\\n\\n (+(+(+(+(+(+ GAN SUMMARY +)+)+)+)+)+) \\n\\n\")\n",
        "        self.discriminator_model.trainable = False # freeze the model because at first, we will train only generator model.\n",
        "        real_input = Input(shape=(self.generator_input_features,)) # the real input features of our gan model \n",
        "        generator_output_tensor = self.generator_model(real_input) # pass input of shape for example 100 neurons to generator model input - output size : (None, self.h, self.w, self.channels)\n",
        "        discriminator_output_tensor = self.discriminator_model(generator_output_tensor) # this is the output tensor of our discriminator model which is the result of passing the output of generator model to it for discriminating - output size : (None , 1)\n",
        "        self.gan = Model(inputs=real_input, outputs=discriminator_output_tensor) # input size : (None, 100) - output size : (None, self.h, self.w, self.channels) and (None, 1) for two model objects\n",
        "        self.gan.compile(loss=HPARAM.loss, optimizer=HPARAM.optimizer('Adam')) # use Adam optimizer to prevent nan loss from happening!\n",
        "        self.gan.summary() # the structure is : one input layer and 2 model objects | data -> generator -> discriminator\n",
        "\n",
        "    def predictNoise(self, b_size):\n",
        "        # noise = tf.random.normal([b_size, self.generator_input_features])\n",
        "        # return self.generator_model.predict(noise, steps=b_size)\n",
        "        noise = np.random.normal(0, 1, (b_size, self.generator_input_features)) # output shape : (25,100) - to match the first layer matrix we suppose a (25, 100) matrix ; cause our first layer has 100 features or neurons\n",
        "        generated_noise = self.generator_model.predict(noise) # input shape : (b_size, 100) to the generator model with 12544 neurons for first hidden layer\n",
        "        # print(f\"\\n\\n[======NONE SCALED GENERATED NOISE======]\\n\\n{generated_noise}\") # the are between [-1, 1]\n",
        "        generated_noise = 0.5 * generated_noise + 0.5 # scale the image which is between -1 and 1 to 0 and 1 - because the output of discriminator is [1, 0] and we have to scale our input data for the network\n",
        "        # print(f\"\\n\\n[======SCALED GENERATED NOISE======]\\n\\n{generated_noise}\")\n",
        "        return generated_noise\n",
        "\n",
        "\n",
        "    def __plotLoss(self, analaysis):\n",
        "        anal = pd.DataFrame(analaysis)\n",
        "        # print(f\"\\n\\n[=========ANALAYSIS DATAFRAME=========]\\n\\n\\t{analaysis}\\n\\n\")\n",
        "        plt.figure(figsize=(20,5))\n",
        "        for col in anal.columns:\n",
        "            plt.plot(anal[col], label=col)\n",
        "        plt.legend()\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.show()\n",
        "\n",
        "    def __saveImages(self, epoch):\n",
        "        generated_noise = self.predictNoise(b_size=25) # predict for 25 noisy images or 25 batch size - output size : (25, self.h, self.w, self.channels)\n",
        "        self.ipkit.saveGANIMG(generated_noise, epoch)\n",
        "    \n",
        "    def fit(self):\n",
        "        '''\n",
        "        G(Z) : generated_noise & D(G(Z)) : discriminating generated_noise.\n",
        "\n",
        "        since we are only training generators here, we do not want to adjust the weights of discriminator.\n",
        "        this is what really an “Adversarial” in Adversarial Network means, if we do not set this,\n",
        "        the generator will get its weight adjusted so it gets better at fooling discriminator \n",
        "        and it also adjusts the weights of the discriminator to make it better at being fooled.\n",
        "        we don’t want this. So, we have to train them separately and fight against each other.\n",
        "\n",
        "        NOTE : for weights matrix of for example discriminator model you might want look at the self.discriminator_model.trainable_weights\n",
        "\n",
        "        Resources :\n",
        "            https://stackoverflow.com/questions/49100556/what-is-the-use-of-train-on-batch-in-keras\n",
        "        '''\n",
        "        real, fake, analaysis = np.ones((HPARAM.batch_size, 1)), np.zeros((HPARAM.batch_size, 1)), []\n",
        "        for epoch in range(HPARAM.epochs):\n",
        "            # train the discriminator\n",
        "            batch_indices = np.random.randint(0, self.x_train.shape[0], HPARAM.batch_size) # select a random batch index in every epoch - from 0 to 60000 select 256 numbers (all in a vector) randomly\n",
        "            ''' \n",
        "            _____________\n",
        "            batch_indices\n",
        "            _____________\n",
        "            array([ 50158, 16409, 30597, 29546,   319,  4291, 51797, 54794,  6347,\n",
        "                     7069, 52617, 49496, 26257, 43211, 55807, 22331, 59974, 30536,\n",
        "                    17414, 40078,  7072, 37538, 14441, 57660,  5116, 21368, 55153,\n",
        "                    43102, 42353, 27893, 29977, 11031,  1616, 11436, 21777,  2332,\n",
        "                    54398,  6234, 18930, 55992, 46033,  9673, 16671, 22962, 20765,\n",
        "                    47814,  3356, 32539,  4383, 41285, 34036, 34127,  9072, 24465,\n",
        "                    47100, 11636, 12967, 23162, 41097,  4734, 20491, 10962, 25151,\n",
        "                    59480,  1141, 25529, 42356, 49382, 48849, 13896, 36950, 56744,\n",
        "                    41305, 28920, 58811,  9976,  4130, 46302, 10651, 22600, 24753,\n",
        "                    26635, 43672,  2597, 20258, 17594, 55960,  2659, 31022, 11578,\n",
        "                    363, 39632,  1849, 54379, 38461, 18003, 20922, 49457, 39769,\n",
        "                    44198, 45235, 39556, 19735, 11482, 58004, 41398, 40015, 14609,\n",
        "                    53160, 38309, 39280, 22937, 53333, 22890,  2079, 53312, 40692,\n",
        "                    13509, 11716, 29095, 44632, 46122, 59922,  2963, 50709, 43212,\n",
        "                    18576, 18836, 24681, 58573, 55646, 29433,  2155, 55819,  5009,\n",
        "                    16998, 24879, 19256, 10684,  5894,  9327, 54049, 20190, 55694,\n",
        "                    15428, 46618, 39944, 29927, 28428, 57629, 16189, 50461, 47883,\n",
        "                    10925, 58968, 31744, 46616, 45884, 53003, 12188, 41767, 22236,\n",
        "                    41064, 51983,  8649,  8371, 33150, 53032, 21250,  5042,  8446,\n",
        "                    17185, 15240, 52005, 56308, 28442, 22945, 17471, 45489, 35707,\n",
        "                    13220, 39728, 16875, 40651, 15097,  9777, 39925, 40075, 32079,\n",
        "                    44707, 35861, 48429,   858, 20081, 55424,  5725, 47555, 56829,\n",
        "                    23527,  7094,  3135, 21713, 47165, 40855, 39863, 53819, 15278,\n",
        "                    49148, 38378, 53356,  4976, 44295, 48315, 16029, 50425, 32885,\n",
        "                    17884, 13242, 29558, 29272, 48546, 19890, 25021, 23387, 29972,\n",
        "                    22101, 38527, 25241, 46481,  8348, 26805, 20334, 24750, 17431,\n",
        "                    37012, 13997, 50870, 36485,  2915, 32686, 26367, 47315, 10140,\n",
        "                    49261, 51778, 20742, 31083, 59523, 16225, 56906, 49414, 58086,\n",
        "                    44808, 51192, 56173,   400])\n",
        "            '''\n",
        "            batch = self.x_train[batch_indices] # get a random set of real images - shape : (256, 28, 28, 1)\n",
        "            batch = 0.5 * batch + 0.5 # rescale to [0, 1] - because all training images have range [-1, 1] and to feed the batch into the discriminator network we have to scale our data to [0, 1]\n",
        "            generated_noise = self.predictNoise(HPARAM.batch_size) # for 256 data we produce noisy images using our generator model with shape (256, 28, 28, 1) \n",
        "            self.discriminator_model.trainable = True # pre train discriminator on fake and real data before starting the gan to let the discriminator model weights update\n",
        "            real_metric_loss = self.discriminator_model.train_on_batch(batch, real) # runs a single gradient update on a single batch of data and returns scalar training loss for real images - how much they are real! train to get the 1s.\n",
        "            fake_metric_loss = self.discriminator_model.train_on_batch(generated_noise, fake)  # runs a single gradient update on a single batch of data and returns scalar training loss for fake images - how much they are fake! train to get the 0s.\n",
        "            discriminator_loss = 0.5 * np.add(real_metric_loss, fake_metric_loss) # in practice, we divide the objective by 2 while optimizing discriminator, which slows down the rate at which discriminator learns relatively to generator.\n",
        "            # train the generator\n",
        "            self.discriminator_model.trainable = False # during the training of gan, the weights of discriminator should be fixed and we can enforce that by setting the trainable flag\n",
        "            noise = np.random.normal(0, 1, (HPARAM.batch_size, self.generator_input_features)) # we'll feed this generated noise into our gan model to produce real images from noisy by training our gen model\n",
        "            generator_metric_loss = self.gan.train_on_batch(noise, real) # training the gan by alternating the training of the discriminator and training the chained gan model with discriminator’s weights freezed ; runs a single gradient update on a single batch of data and returns scalar training loss - take a (batch_size, 100) matrix as input and (batch_size, 1) filled with 1 matrix as real output value ; our gan model has (batch_size, 100) -> (batch_size, 28, 82, 1) -> (batch_size, 1) architecture\n",
        "            \n",
        "            print(f\"[*************EPOCH - {epoch}*************]\")\n",
        "            print(f\"DISCRIMINATOR LOSS ⏎\\n\\t{discriminator_loss[0]}\\n\")\n",
        "            print(f\"DISCRIMINATOR ACC ⏎\\n\\t{discriminator_loss[1]*100}\\n\")\n",
        "            print(f\"GENERATOR LOSS ⏎\\n\\t{generator_metric_loss}\\n\")\n",
        "            print(\"_________________________________________________________________________________________________________\\n\")\n",
        "            \n",
        "            analaysis.append({\"D\": discriminator_loss[0], \"G\": generator_metric_loss})\n",
        "            if epoch % 10 == 0:\n",
        "                self.__saveImages(epoch)\n",
        "        self.__plotLoss(analaysis)\n",
        "        self.ipkit.MakeGif() \n",
        "            \n",
        "\n",
        "class HPARAM:\n",
        "    loss = 'binary_crossentropy'\n",
        "    optimizer = lambda name : Adam(lr=0.0002, beta_1=0.5) if name == 'Adam' else RMSprop(learning_rate=0.0008, rho=1.0, decay=6e-8) \n",
        "    batch_size = 256\n",
        "    epochs = 30000\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    dropout = 0.4\n",
        "    momentum = 0.9\n",
        "    depth = 256\n",
        "    metrics = ['accuracy']\n",
        "\n",
        "\n",
        "\n",
        "gan = GAN(dataset_name='paint', channels=3, generator_input_features=100, discNetwork='dcgan') # discriminator network : dcgan or cgan | dataset_name : paint, fashion_mnist, mnist\n",
        "gan.fit() # start training\n",
        "noise = gan.predictNoise(b_size=25) # it should give us a real image! models predictions.\n",
        "print(\"\\n\\n |=> DISCRIMINATING 25 BATCHES <=|\\n\\n{}\".format(gan.discriminator.predict(noise)))\n",
        "print(\"\\n\\n |=> 0th GENERATED NOISE FROM GENERATOR <=|\")\n",
        "plt.imshow(noise[0, :, :, 0] * 127.5 + 127.5, cmap='gray') # plot the 0th predicted noise - because of generator output we have to scale the prediction to [0, 255], so we multiply by 127.5 and add 127.5\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QJvyyAaotS1",
        "colab_type": "text"
      },
      "source": [
        "**DCGAN Generated Noise After 30K Epochs On MNIST Dataset**\n",
        "\n",
        "![dcgan gif]()\n",
        "\n",
        "**Capsule GAN Generated Noise After 30K Epochs On MNIST Dataset**\n",
        "\n",
        "![cgan gif]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiRoxRoVOX4a",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Stack-GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxChbo8bOetO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "https://medium.com/@mrgarg.rajat/implementing-stackgan-using-keras-a0a1b381125e\n",
        "'''\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# Generating Images From Text - Implementing Stack-GAN\n",
        "# ====================================================\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-jbFqvwxJbM",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ SRGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QE1_aRFxKPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://medium.com/@birla.deepak26/single-image-super-resolution-using-gans-keras-aca310f33112\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i4j6hoamlfv",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Style-GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV-ihSNem55-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://github.com/NVlabs/stylegan\n",
        "'''\n",
        "\n",
        "# ====================================================\n",
        "# Style Generative Adversarial Network - Version 1 & 2\n",
        "# ====================================================\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_NPOpsPCcGV",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Progressive Growing-GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFzWBSZcCnSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "http://bamos.github.io/2016/08/09/deep-completion/#image-completion-with-dcgans\n",
        "https://machinelearningmastery.com/how-to-implement-progressive-growing-gan-models-in-keras/\n",
        "'''\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "#  Progressive Growing GAN - Image Completion\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s4uScLJXJa9",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ tfgan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0tqIAIuXaip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://developers.google.com/machine-learning/gan/\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzLsvst-mQ5w",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ AAE/VAE**\n",
        "\n",
        "![Autoencoder](https://drive.google.com/uc?id=1JzT575p8iwiJZT9Mpn7vnahQwLKKu5sA)\n",
        "\n",
        "**Tip :** *Autoencoders learns features by decoding the coded input to produce the its input.*\n",
        "\n",
        "![aae](https://drive.google.com/uc?id=14LoBHyPwdo9u2sXixp7Yc5NHVgr2bgih)\n",
        "\n",
        "![vae](https://drive.google.com/uc?id=1LM1sDU9foCHRwj9Baij2OO_ji63SCWK8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqq3fqWOmWeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://www.groundai.com/project/stacked-capsule-autoencoders/1\n",
        "https://www.youtube.com/watch?v=Cc1DEY12r5E\n",
        "https://medium.com/syncedreview/geoffrey-hintons-unsupervised-capsule-networks-achieve-sota-results-on-svhn-ffe05e871249\n",
        "http://akosiorek.github.io/ml/2019/06/23/stacked_capsule_autoencoders.html\n",
        "https://github.com/eriklindernoren/Keras-GAN/blob/master/aae/aae.py\n",
        "https://medium.com/ai%C2%B3-theory-practice-business/war-on-deepfakes-a-new-and-effective-way-of-detecting-forged-images-3ae94df80769\n",
        "https://medium.com/gradientcrescent/deepfaking-nicolas-cage-into-the-mcu-using-autoencoders-an-implementation-in-keras-and-tensorflow-ab47792a042f\n",
        "https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4\n",
        "https://zzutk.github.io/Face-Aging-CAAE/\n",
        "https://medium.com/@a.mirzaei69/adversarial-autoencoders-on-mnist-dataset-python-keras-implementation-5eeafd52ab21\n",
        "https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368\n",
        "https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788292061/10/ch10lvl1sec82/variational-autoencoder-in-keras\n",
        "https://towardsdatascience.com/vaes-generating-images-with-tensorflow-61de08e82f1f\n",
        "https://www.machinecurve.com/index.php/2019/12/30/how-to-create-a-variational-autoencoder-with-keras/\n",
        "https://www.kaggle.com/vikramtiwari/autoencoders-using-tf-keras-mnist\n",
        "https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7\n",
        "https://keras.io/examples/variational_autoencoder/\n",
        "https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/\n",
        "https://www.tensorflow.org/tutorials/generative/cvae\n",
        "https://towardsdatascience.com/unlocking-drug-discovery-through-machine-learning-part-1-8b2a64333e07\n",
        "https://www.tensorflow.org/tutorials/generative/cvae\n",
        "https://medium.com/ai%C2%B3-theory-practice-business/anomaly-detection-part-1-autoencoder-58bdbbea5001\n",
        "https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/\n",
        "https://github.com/search?l=Python&q=neural+drug&type=Repositories\n",
        "https://github.com/topazape/LSTM_Chem\n",
        "https://bayeslabs.co/\n",
        "https://blog.bayeslabs.co/2019/06/08/Adversarial-Autoencoders-for-Molecular-Generation.html\n",
        "https://medium.com/neuromation-blog/creating-molecules-from-scratch-i-drug-discovery-with-generative-adversarial-networks-9d42cc496fc6\n",
        "https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "https://machinelearningmastery.com/lstm-autoencoders/\n",
        "https://duvenaud.github.io/learn-discrete/slides/AdversarialAutoencoders.pdf\n",
        "https://rubikscode.net/2019/01/21/generating-images-using-adversarial-autoencoders-and-python/\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Generating & Reconstructing Images using Adversarial/Variational Autoencoders\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "# =====================================================================================================\n",
        "# LSTM/GRU Adversarial/Variational Autoencoiders - Generating Full Meaning Form of an Abbreviation Word\n",
        "# =====================================================================================================\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Anomaly Detection using Adversarial/Variational Autoencoders\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "# =======================================================================================\n",
        "# Drug Discovery (COVID-19) using Conditional Adversarial/Variational Autoencoders & LSTM\n",
        "# =======================================================================================\n",
        "\n",
        "\n",
        "# ===================================================================================\n",
        "# LSTM/GRU Adversarial/Variational Autoencoders - Generating Text with GAN & LSTM/GRU\n",
        "# ===================================================================================\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# LSTM/GRU Adversarial/Variational Autoencoiders - Hashing Text with GAN & LSTM/GRU\n",
        "# =================================================================================\n",
        "\n",
        "\n",
        "# ==========================================================================================\n",
        "# Deep Convolutional Adversarial/Variational Autoencoders - Reconstructing/Generating images\n",
        "# ==========================================================================================\n",
        "\n",
        "\n",
        "# ==========================================================================================\n",
        "# Stacked CapsuleNet Adversarial/Variational Autoencoders - Reconstructing/Generating images\n",
        "# ==========================================================================================\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# Anomaly Detection using Adversarial Dual Autoencoders\n",
        "# =====================================================\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jIlL767bM6j",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | Autoregressive Networks _ PixelRNN & PixelCNN++**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7QCFzJadB1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://towardsdatascience.com/summary-of-pixelrnn-by-google-deepmind-7-min-read-938d9871d6d9\n",
        "https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173\n",
        "https://medium.com/intuitionmachine/voice-style-transfer-using-deep-learning-d173f1608af5\n",
        "https://www.oreilly.com/content/using-tensorflow-to-generate-images-with-pixelrnns/\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM2GRt6A_nD-",
        "colab_type": "text"
      },
      "source": [
        "# **Generate Art Using Neural Style Transfer & DeepDream**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzqPyZimBAbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://www.tensorflow.org/tutorials/generative/style_transfer\n",
        "https://www.tensorflow.org/tutorials/generative/deepdream\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsSxs0C0bgr7",
        "colab_type": "text"
      },
      "source": [
        "# **Generate Art Using Pix2Pix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVgsx16GbmOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/\n",
        "https://www.tensorflow.org/tutorials/generative/pix2pix\n",
        "https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/\n",
        "https://modelzoo.co/model/pix2pix-keras\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}