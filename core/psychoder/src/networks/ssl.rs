


/*

https://www.lpalmieri.com/posts/2019-03-12-scientific-computing-a-rust-adventure-part-1-zero-cost-abstractions/
https://www.lpalmieri.com/posts/2019-02-23-scientific-computing-a-rust-adventure-part-0-vectors/
https://www.lpalmieri.com/posts/2019-04-07-scientific-computing-a-rust-adventure-part-2-array1/
https://github.com/wildonion/uniXerr/blob/master/core/recognizer/helper_board

NOTE - feature vectoers learning and representation or extracting semantic attributes using dimensionality reduction algorithms like GA, TSNE, PCA and VAE 
        to compare them with the new unseen input vectors using their distance based on a treshhold between the feature vector representation of the 
        unseen input and the training input embedding vectors model to understand the similarity and abnormality like incoming new category 
        in verification tasks and the category that the new input must be in in classification tasks. 

➔ zero shot learning like clip, dalle and gpt which generate the final result from the embedding generated by an encoder and contains the semantic attributes layer using a decoder
➔ train input -> feature extractor -> feature vectors for each training input in a latent space or an embedding layer of extracted feature 
➔ unseen test input -> feature extractor -> if | unseen feature vectors - each training feature vectors | > treshhold :: abnormality or a new category or label for dataset else unseen input belongs to the smallest distance class
➔ text -> embedding vector -> CLIP model -> generate latent image embedding by extracting semantic attributes and features -> decoder that generates an image conditioned on the latent image embedding 
➔ in ssl or meta learning we're looking for finding the semantic meaning between input features and put them into a latent space or an embedding layer of extracted feature

*/

pub mod transformers;
pub mod gan;
pub mod vae;
